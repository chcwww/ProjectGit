{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/chcwww/ProjectGit/blob/main/method/featureSelect/featureFunction.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2gx94fTPAsyy",
        "outputId": "f624a62c-6b17-4508-c889-6ec06e29d7ec"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "\n",
        "offers = pd.read_csv(r'/content/drive/MyDrive/1經濟學/專題/offers.csv')\n",
        "transactions = pd.read_csv(r'/content/drive/MyDrive/1經濟學/專題/newdata.csv')\n",
        "trainhistory = pd.read_csv(r'/content/drive/MyDrive/1經濟學/專題/trainHistory.csv')\n",
        "testhistory = pd.read_csv(r'/content/drive/MyDrive/1經濟學/專題/testHistory.csv')"
      ],
      "metadata": {
        "id": "OTUUDNeZAzv-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# transactions.info(memory_usage='deep')"
      ],
      "metadata": {
        "id": "OQ4-pG8zIAXN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tranDtype = {'id': 'uint64',\n",
        " 'chain': 'uint16',\n",
        " 'dept': 'uint8',\n",
        " 'category': 'uint16',\n",
        " 'company': 'uint64',\n",
        " 'brand': 'uint32',\n",
        "#  'date' : 'object',\n",
        " 'productsize': 'float32',\n",
        " 'productmeasure': 'category',\n",
        " 'purchasequantity': 'int64',\n",
        " 'purchaseamount': 'float32'}"
      ],
      "metadata": {
        "id": "DFVU6nIEYkS3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# tranDtype1 = {\n",
        "#  'category': 'uint16',\n",
        "#  'brand': 'uint32',\n",
        "#  'date' : 'category',\n",
        "#  'productsize': 'float32',\n",
        "#  'purchasequantity': 'int64',\n",
        "#  'purchaseamount': 'float32'\n",
        "#  }"
      ],
      "metadata": {
        "id": "TQ7hbkDgcSK7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "# from dask import dataframe as dd\n",
        "transactions = pd.read_csv('/content/drive/MyDrive/1經濟學/專題/transactions.csv', \\\n",
        "    # names=['id', 'chain', 'dept', 'category', 'company', 'brand', 'date', \\\n",
        "    # 'productsize', 'productmeasure', 'purchasequantity', 'purchaseamount'],\n",
        "    dtype = tranDtype,\n",
        "    nrows = 100000,\n",
        "    parse_dates=['date'],\n",
        "    infer_datetime_format=True\n",
        "    # blocksize=64000000 # = 64 Mb chunks\n",
        ")"
      ],
      "metadata": {
        "id": "vAoOA4VOGo39"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transactions.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "j5AYZZXTCrFy",
        "outputId": "3f2e076f-d105-4b59-8055-3f359ffc32d5"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(10000, 11)"
            ]
          },
          "metadata": {},
          "execution_count": 45
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def find_index(data_col, val):\n",
        "    val_list = []\n",
        "    \n",
        "    val_list.append(val)\n",
        "    val_list.append(\"end\")\n",
        "\n",
        "    index = data_col.isin(val_list).idxmax()\n",
        "    \n",
        "    return index"
      ],
      "metadata": {
        "id": "GSdlrYwtE78W"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "idx = find_index(transactions['id'], int(transactions['id'].loc[len(transactions['id']) - 1])) "
      ],
      "metadata": {
        "id": "nycDReWmEY9F"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "transactions['id'].loc[idx]"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AbsLqkoJFN50",
        "outputId": "3cd4f89d-31d0-485a-b0a0-60111e280060"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "126082871"
            ]
          },
          "metadata": {},
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "transactions.info(memory_usage='deep')"
      ],
      "metadata": {
        "id": "cFp6MZLsJkUH",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b1edb27-4401-4caf-9f61-d86c9026bd03"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'pandas.core.frame.DataFrame'>\n",
            "RangeIndex: 15349956 entries, 0 to 15349955\n",
            "Data columns (total 11 columns):\n",
            " #   Column            Dtype  \n",
            "---  ------            -----  \n",
            " 0   id                int64  \n",
            " 1   chain             int64  \n",
            " 2   dept              int64  \n",
            " 3   category          int64  \n",
            " 4   company           int64  \n",
            " 5   brand             int64  \n",
            " 6   date              object \n",
            " 7   productsize       float64\n",
            " 8   productmeasure    object \n",
            " 9   purchasequantity  int64  \n",
            " 10  purchaseamount    float64\n",
            "dtypes: float64(2), int64(7), object(2)\n",
            "memory usage: 2.8 GB\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# import gzip\n",
        "# with gzip.open('/content/drive/MyDrive/1經濟學/專題/transactions.csv.gz', 'rb') as f:\n",
        "#   file_content = f.read()"
      ],
      "metadata": {
        "id": "9ovZxIY3DhCk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Feature Function"
      ],
      "metadata": {
        "id": "xov93lgoguMu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def generateFeature(offers, transactions, trainHistory, testHistory) :\n",
        "# import useful package\n",
        "  from sklearn.preprocessing import OrdinalEncoder, LabelEncoder\n",
        "  from datetime import timedelta\n",
        "  from itertools import cycle\n",
        "  import pandas as pd\n",
        "  import numpy as np\n",
        "  import time\n",
        "  import gc\n",
        "  ts = time.time()\n",
        "  # tk = 1\n",
        "  \n",
        "  # get all data & delete those not in transactions\n",
        "  train = trainHistory.drop(columns = ['repeater', 'repeattrips'])\n",
        "  data = pd.concat([train, testHistory], axis=0, ignore_index=True)\n",
        "  use = data[data['id'].isin(transactions['id'])] # (310665, )\n",
        "\n",
        "  # del trainHistory, train, testHistory\n",
        "  gc.collect()\n",
        "  \n",
        "  # te = time.time()\n",
        "  # print('Inner -', str(tidx) + ' of ' + str(tk) + '(get all data) -> time elapsed: ' + str(round(te-ts, 2)) + ' seconds')\n",
        "  # tk+=1\n",
        "  \n",
        "  # put offer information into transactions\n",
        "  of = offers[['offer', 'category', 'company', 'offervalue', 'brand']]\n",
        "  usf = pd.merge(use, of, on='offer')\n",
        "  usf.columns = ['id', 'chain', 'offer', 'market', 'offerdate', 'offercategory', 'offercompany',\n",
        "        'offervalue', 'offerbrand']\n",
        "  tu = usf[['id', 'offer', 'offerdate', 'offercategory', 'offercompany', 'offerbrand']]\n",
        "  nu = pd.merge(tu, transactions, on='id')\n",
        "  \n",
        "  del tu, usf, of, transactions\n",
        "  gc.collect()\n",
        "  \n",
        "  # te = time.time()\n",
        "  # print('Inner -', str(tidx) + ' of ' + str(tk) + '(put offer information) -> time elapsed: ' + str(round(te-ts, 2)) + ' seconds')\n",
        "  # tk+=1\n",
        "  \n",
        "  # generate time index\n",
        "  date_format = '%Y-%m-%d'\n",
        "  nu['offerdate'] = pd.to_datetime(nu['offerdate'], format = date_format)\n",
        "  nu['date'] = pd.to_datetime(nu['date'], format = date_format)\n",
        "  nu['daydiff'] = nu['offerdate'] - nu['date']\n",
        "  nu['diff_180'] = nu['offerdate'] - timedelta(days = 180)\n",
        "  nu['diff_150'] = nu['offerdate'] - timedelta(days = 150)\n",
        "  nu['diff_120'] = nu['offerdate'] - timedelta(days = 120)\n",
        "  nu['diff_90'] = nu['offerdate'] - timedelta(days = 90)\n",
        "  nu['diff_60'] = nu['offerdate'] - timedelta(days = 60)\n",
        "  nu['diff_30'] = nu['offerdate'] - timedelta(days = 30)\n",
        "  \n",
        "  # te = time.time()\n",
        "  # print('Inner -', str(tidx) + ' of ' + str(tk) + '(generate time index) -> time elapsed: ' + str(round(te-ts, 2)) + ' seconds')\n",
        "  # tk+=1\n",
        "  \n",
        "  # put offervalue\n",
        "  of1 = offers[['offer', 'offervalue']]\n",
        "  use = pd.merge(use, of1, on='offer')\n",
        "  \n",
        "  # te = time.time()\n",
        "  # print('Inner -', str(tidx) + ' of ' + str(tk) + '(put offervalue) -> time elapsed: ' + str(round(te-ts, 2)) + ' seconds')\n",
        "  # tk+=1\n",
        "  \n",
        "  # generate total\n",
        "  group = nu.groupby(['id'])\n",
        "  \n",
        "  test = group['chain'].count().reset_index()\n",
        "  test.columns = ['id', 'buy_total_freq']\n",
        "  use = pd.merge(use, test, on='id')\n",
        "  \n",
        "  # te = time.time()\n",
        "  # print('Inner -', str(tidx) + ' of ' + str(tk) + '(generate total freq) -> time elapsed: ' + str(round(te-ts, 2)) + ' seconds')\n",
        "  # tk+=1\n",
        "  \n",
        "  test = group['purchaseamount'].sum().reset_index()\n",
        "  test.columns = ['id', 'buy_total_amount']\n",
        "  use = pd.merge(use, test, on='id')\n",
        "  \n",
        "  # te = time.time()\n",
        "  # print('Inner -', str(tidx) + ' of ' + str(tk) + '(generate total amount) -> time elapsed: ' + str(round(te-ts, 2)) + ' seconds')\n",
        "  # tk+=1\n",
        "  \n",
        "  test = group['purchaseamount'].mean().reset_index()\n",
        "  test.columns = ['id', 'buy_total_avgamount']\n",
        "  use = pd.merge(use, test, on='id')\n",
        "  \n",
        "  # te = time.time()\n",
        "  # print('Inner -', str(tidx) + ' of ' + str(tk) + '(generate total avg) -> time elapsed: ' + str(round(te-ts, 2)) + ' seconds')\n",
        "  # tk+=1\n",
        "  \n",
        "  test = group['purchasequantity'].sum().reset_index()\n",
        "  test.columns = ['id', 'buy_total_quantity']\n",
        "  use = pd.merge(use, test, on='id')\n",
        "  \n",
        "  # te = time.time()\n",
        "  # print('Inner -', str(tidx) + ' of ' + str(tk) + '(generate total quan) -> time elapsed: ' + str(round(te-ts, 2)) + ' seconds')\n",
        "  # tk+=1\n",
        "  \n",
        "  test = group['daydiff'].min().reset_index()\n",
        "  test.columns = ['id', 'buy_total_daydiff']\n",
        "  use = pd.merge(use, test, on='id')\n",
        "  \n",
        "  # te = time.time()\n",
        "  # print('Inner -', str(tidx) + ' of ' + str(tk) + '(generate total) -> time elapsed: ' + str(round(te-ts, 2)) + ' seconds')\n",
        "  # tk+=1\n",
        "  \n",
        "  \n",
        "  day = np.linspace(30, 180, 6, endpoint=True).astype(int).astype(str)\n",
        "  \n",
        "  for i in day :\n",
        "    daa = 'diff_' + i\n",
        "    nu['ascom'] = nu['date'] >= nu[daa]\n",
        "  \n",
        "    name = 'buy_total_amount_' + i\n",
        "    var = 'purchaseamount'\n",
        "  \n",
        "    group = nu.groupby(['id', 'ascom'])\n",
        "    test = group[var].sum().reset_index()\n",
        "    test.columns = ['id', 'ascom', name]\n",
        "    use = pd.merge(use, test[test['ascom']][['id', name]], on='id', how = 'outer')\n",
        "  \n",
        "    # te = time.time()\n",
        "    # print('Inner -', str(tidx) + ' of ' + str(tk) + '(generate day amount) -> time elapsed: ' + str(round(te-ts, 2)) + ' seconds')\n",
        "    # tk+=1\n",
        "  \n",
        "    name = 'buy_total_quantity_' + i\n",
        "    var = 'purchasequantity'\n",
        "  \n",
        "    group = nu.groupby(['id', 'ascom'])\n",
        "    test = group[var].sum().reset_index()\n",
        "    test.columns = ['id', 'ascom', name]\n",
        "    use = pd.merge(use, test[test['ascom']][['id', name]], on='id', how = 'outer')\n",
        "  \n",
        "    # te = time.time()\n",
        "    # print('Inner -', str(tidx) + ' of ' + str(tk) + '(generate day quan) -> time elapsed: ' + str(round(te-ts, 2)) + ' seconds')\n",
        "    # tk+=1\n",
        "  \n",
        "    name = 'buy_total_freq_' + i\n",
        "    var = 'purchaseamount'\n",
        "  \n",
        "    group = nu.groupby(['id', 'ascom'])\n",
        "    test = group['chain'].count().reset_index()\n",
        "    test.columns = ['id', 'ascom', name]\n",
        "    use = pd.merge(use, test[test['ascom']][['id', name]], on='id', how = 'outer')\n",
        "  \n",
        "    # te = time.time()\n",
        "    # print('Inner -', str(tidx) + ' of ' + str(tk) + '(generate day) -> time elapsed: ' + str(round(te-ts, 2)) + ' seconds')\n",
        "    # tk+=1\n",
        "  \n",
        "  # generate company & brand & category\n",
        "  mea = ['company', 'brand', 'category']\n",
        "  day = np.repeat(np.linspace(30, 180, 6, endpoint=True), 3).astype(int).astype(str)\n",
        "  \n",
        "  for i in mea :\n",
        "    nu['ascom'] = nu[i] == nu['offer' + i]\n",
        "    group = nu.groupby(['id', 'ascom'])\n",
        "  \n",
        "    test = group['chain'].count().reset_index()\n",
        "    test.columns = ['id', 'ascom', 'buy_'+i+'_freq']\n",
        "    use = pd.merge(use, test[test['ascom']][['id', 'buy_'+i+'_freq']], on='id', how = 'outer')\n",
        "  \n",
        "    # te = time.time()\n",
        "    # print('Inner -', str(tidx) + ' of ' + str(tk) + '(generate other freq) -> time elapsed: ' + str(round(te-ts, 2)) + ' seconds')\n",
        "    # tk+=1\n",
        "  \n",
        "    test = group['purchaseamount'].sum().reset_index()\n",
        "    test.columns = ['id', 'ascom', 'buy_'+i+'_amount']\n",
        "    use = pd.merge(use, test[test['ascom']][['id', 'buy_'+i+'_amount']], on='id', how = 'outer')\n",
        "  \n",
        "    # te = time.time()\n",
        "    # print('Inner -', str(tidx) + ' of ' + str(tk) + '(generate other amount) -> time elapsed: ' + str(round(te-ts, 2)) + ' seconds')\n",
        "    # tk+=1\n",
        "  \n",
        "    test = group['purchaseamount'].mean().reset_index()\n",
        "    test.columns = ['id', 'ascom', 'buy_'+i+'_avgamount']\n",
        "    use = pd.merge(use, test[test['ascom']][['id', 'buy_'+i+'_avgamount']], on='id', how = 'outer')\n",
        "  \n",
        "    # te = time.time()\n",
        "    # print('Inner -', str(tidx) + ' of ' + str(tk) + '(generate other avg) -> time elapsed: ' + str(round(te-ts, 2)) + ' seconds')\n",
        "    # tk+=1\n",
        "  \n",
        "    test = group['purchasequantity'].sum().reset_index()\n",
        "    test.columns = ['id', 'ascom', 'buy_'+i+'_quantity']\n",
        "    use = pd.merge(use, test[test['ascom']][['id', 'buy_'+i+'_quantity']], on='id', how = 'outer')\n",
        "  \n",
        "    # te = time.time()\n",
        "    # print('Inner -', str(tidx) + ' of ' + str(tk) + '(generate other quan) -> time elapsed: ' + str(round(te-ts, 2)) + ' seconds')\n",
        "    # tk+=1\n",
        "  \n",
        "    test = group['daydiff'].min().reset_index()\n",
        "    test.columns = ['id', 'ascom', 'buy_'+i+'_daydiff']\n",
        "    use = pd.merge(use, test[test['ascom']][['id', 'buy_'+i+'_daydiff']], on='id', how = 'outer')\n",
        "  \n",
        "    # te = time.time()\n",
        "    # print('Inner -', str(tidx) + ' of ' + str(tk) + '(generate other) -> time elapsed: ' + str(round(te-ts, 2)) + ' seconds')\n",
        "    # tk+=1\n",
        "  \n",
        "  \n",
        "  for i, j in zip(day, cycle(mea)) :\n",
        "    daa = 'diff_' + i\n",
        "    nu['ascom'] = ((nu['date'] >= nu[daa]) & (nu[j] == nu['offer' + j]))\n",
        "  \n",
        "    name = 'buy_'+j+'_amount_' + i\n",
        "    var = 'purchaseamount'\n",
        "  \n",
        "    group = nu.groupby(['id', 'ascom'])\n",
        "    test = group[var].sum().reset_index()\n",
        "    test.columns = ['id', 'ascom', name]\n",
        "    use = pd.merge(use, test[test['ascom']][['id', name]], on='id', how = 'outer')\n",
        "  \n",
        "    # te = time.time()\n",
        "    # print('Inner -', str(tidx) + ' of ' + str(tk) + '(generate other day amount) -> time elapsed: ' + str(round(te-ts, 2)) + ' seconds')\n",
        "    # tk+=1\n",
        "  \n",
        "    name = 'buy_'+j+'_quantity_' + i\n",
        "    var = 'purchasequantity'\n",
        "  \n",
        "    group = nu.groupby(['id', 'ascom'])\n",
        "    test = group[var].sum().reset_index()\n",
        "    test.columns = ['id', 'ascom', name]\n",
        "    use = pd.merge(use, test[test['ascom']][['id', name]], on='id', how = 'outer')\n",
        "  \n",
        "    # te = time.time()\n",
        "    # print('Inner -', str(tidx) + ' of ' + str(tk) + '(generate other day quan) -> time elapsed: ' + str(round(te-ts, 2)) + ' seconds')\n",
        "    # tk+=1\n",
        "  \n",
        "    name = 'buy_'+j+'_freq_' + i\n",
        "    var = 'purchaseamount'\n",
        "  \n",
        "    group = nu.groupby(['id', 'ascom'])\n",
        "    test = group['chain'].count().reset_index()\n",
        "    test.columns = ['id', 'ascom', name]\n",
        "    use = pd.merge(use, test[test['ascom']][['id', name]], on='id', how = 'outer')\n",
        "  \n",
        "    # te = time.time()\n",
        "    # # print('Inner -', str(tidx) + ' of ' + str(tk) + '(generate other day) -> time elapsed: ' + str(round(te-ts, 2)) + ' seconds')\n",
        "    # # tk+=1\n",
        "  \n",
        "  # generate not buy index\n",
        "  nu['ascom'] = (nu['company'] == nu['offercompany']) & (nu['brand'] == nu['offerbrand']) & (nu['category'] == nu['offercategory'])\n",
        "  group = nu.groupby(['id', 'ascom'])\n",
        "\n",
        "  name1 = 'buy_company_brand_category'\n",
        "  name = name1 + '_freq'\n",
        "\n",
        "  test = group['chain'].count().reset_index()\n",
        "  test.columns = ['id', 'ascom', name]\n",
        "  use = pd.merge(use, test[test['ascom']][['id', name]], on='id', how = 'outer')\n",
        "  new = pd.DataFrame((use[name] > 0) != True)\n",
        "  new.columns = ['not_' + name1]\n",
        "  use = pd.concat([use, new], axis = 1)\n",
        "\n",
        "\n",
        "\n",
        "  nu['ascom'] = (nu['company'] == nu['offercompany']) & (nu['brand'] == nu['offerbrand'])\n",
        "  group = nu.groupby(['id', 'ascom'])\n",
        "\n",
        "  name1 = 'buy_company_brand'\n",
        "  name = name1 + '_freq'\n",
        "\n",
        "  test = group['chain'].count().reset_index()\n",
        "  test.columns = ['id', 'ascom', name]\n",
        "  use = pd.merge(use, test[test['ascom']][['id', name]], on='id', how = 'outer')\n",
        "  new = pd.DataFrame((use[name] > 0) != True)\n",
        "  new.columns = ['not_' + name1]\n",
        "  use = pd.concat([use, new], axis = 1)\n",
        "\n",
        "\n",
        "\n",
        "  nu['ascom'] = (nu['company'] == nu['offercompany']) & (nu['category'] == nu['offercategory'])\n",
        "  group = nu.groupby(['id', 'ascom'])\n",
        "\n",
        "  name1 = 'buy_company_category'\n",
        "  name = name1 + '_freq'\n",
        "\n",
        "  test = group['chain'].count().reset_index()\n",
        "  test.columns = ['id', 'ascom', name]\n",
        "  use = pd.merge(use, test[test['ascom']][['id', name]], on='id', how = 'outer')\n",
        "  new = pd.DataFrame((use[name] > 0) != True)\n",
        "  new.columns = ['not_' + name1]\n",
        "  use = pd.concat([use, new], axis = 1)\n",
        "\n",
        "\n",
        "\n",
        "  nu['ascom'] = (nu['brand'] == nu['offerbrand']) & (nu['category'] == nu['offercategory'])\n",
        "  group = nu.groupby(['id', 'ascom'])\n",
        "\n",
        "  name1 = 'buy_brand_category'\n",
        "  name = name1 + '_freq'\n",
        "\n",
        "  test = group['chain'].count().reset_index()\n",
        "  test.columns = ['id', 'ascom', name]\n",
        "  use = pd.merge(use, test[test['ascom']][['id', name]], on='id', how = 'outer')\n",
        "  new = pd.DataFrame((use[name] > 0) != True)\n",
        "  new.columns = ['not_' + name1]\n",
        "  use = pd.concat([use, new], axis = 1)\n",
        "\n",
        "  del new\n",
        "  gc.collect()\n",
        "\n",
        "  new1 = pd.DataFrame((use['buy_company_freq'] > 0) != True)\n",
        "  new1.columns = ['not_buy_company']\n",
        "  new2 = pd.DataFrame((use['buy_brand_freq'] > 0) != True)\n",
        "  new2.columns = ['not_buy_brand']\n",
        "  new3 = pd.DataFrame((use['buy_category_freq'] > 0) != True)\n",
        "  new3.columns = ['not_buy_category']\n",
        "  use = pd.concat([use, new1, new2, new3], axis = 1)\n",
        "\n",
        "  del new1, new2, new3\n",
        "  gc.collect()\n",
        "\n",
        "  # handle na problem\n",
        "  dayVar = ['buy_company_daydiff', 'buy_brand_daydiff', 'buy_category_daydiff', 'buy_total_daydiff']\n",
        "  use1 = use[dayVar]\n",
        "  use.drop(columns = dayVar, inplace = True)\n",
        "  use = use.fillna(0)\n",
        "  use1 = use1.fillna(timedelta(0))\n",
        "  use = pd.concat([use, use1], axis = 1)\n",
        "\n",
        "  del use1\n",
        "  gc.collect()\n",
        "  \n",
        "  # te = time.time()\n",
        "  # print('Inner -', str(tidx) + ' of ' + str(tk) + '(handle na problem) -> time elapsed: ' + str(round(te-ts, 2)) + ' seconds')\n",
        "  # tk+=1\n",
        "  \n",
        "  # transform date type into int\n",
        "  for i in use[dayVar] :\n",
        "    use[i] = use[i].astype('str').apply(lambda x:x[:-5]).astype('int32')\n",
        "  \n",
        "  # te = time.time()\n",
        "  # print('Inner -', str(tidx) + ' of ' + str(tk) + '(transform date type) -> time elapsed: ' + str(round(te-ts, 2)) + ' seconds')\n",
        "  # tk+=1\n",
        "  \n",
        "  # transform bool type into int\n",
        "  for i in use.columns :\n",
        "    if(use[i].dtypes == 'bool') :\n",
        "      use[i] = use[i].apply(int)\n",
        "  \n",
        "  te = time.time()\n",
        "  # print('Inner -', str(tidx) + ' (generate feature) -> time elapsed: ' + str(round(te-ts, 2)) + ' seconds')\n",
        "\n",
        "  # transform into train and test\n",
        "  trainData = use[use['id'].isin(trainHistory['id'])] # (159700,\n",
        "  testData = use[use['id'].isin(testHistory['id'])] # (150965\n",
        "  rep = trainHistory[['id', 'repeater']].copy()\n",
        "  rept = trainHistory[['id', 'repeattrips']].copy()\n",
        "  target = rep[rep['id'].isin(trainData['id'])] # (159700,\n",
        "  targetT = rept[rept['id'].isin(trainData['id'])] # (159700,\n",
        "\n",
        "  # ordinal encode\n",
        "  cod = OrdinalEncoder()\n",
        "  cod = cod.fit(use[['market', 'offer', 'chain']])\n",
        "  use[['market', 'offer', 'chain']] = cod.transform(use[['market', 'offer', 'chain']])\n",
        "  trainEncode = use[use['id'].isin(trainHistory['id'])] # (159700,\n",
        "  testEncode = use[use['id'].isin(testHistory['id'])] # (150965\n",
        "\n",
        "  # encode for target\n",
        "  targetEncode = target.copy()\n",
        "  cod = LabelEncoder()\n",
        "  targetEncode['repeater'] = cod.fit_transform(targetEncode['repeater'])\n",
        "  \n",
        "  return trainData, testData, target, targetT, trainEncode, testEncode, targetEncode\n"
      ],
      "metadata": {
        "id": "NGPqJTbDA-bW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Transform into DATA"
      ],
      "metadata": {
        "id": "Qa_-hy1Zgqvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "trainData, testData, target, targetT, trainEncode, testEncode, targetEncode \\\n",
        " = generateFeature(offers, transactions, trainhistory, testhistory)"
      ],
      "metadata": {
        "id": "0LW5OpjUCUeJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trainData.shape"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4uBnks6Dd9LD",
        "outputId": "39e4fb13-84c2-4b6d-e660-f9aac646a467"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(159700, 109)"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "for i in trainData.columns :\n",
        "  print('\"' + i + '\", ')"
      ],
      "metadata": {
        "id": "00RZWKXliZGP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Standardization"
      ],
      "metadata": {
        "id": "PTXPzHCDgg6K"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "select = [\n",
        "    \"buy_total_freq\", \n",
        "    \"buy_total_amount\", \n",
        "    \"buy_total_avgamount\", \n",
        "    \"buy_total_quantity\", \n",
        "    \"buy_total_amount_30\", \n",
        "    \"buy_total_quantity_30\", \n",
        "    \"buy_total_freq_30\", \n",
        "    \"buy_total_amount_60\", \n",
        "    \"buy_total_quantity_60\", \n",
        "    \"buy_total_freq_60\", \n",
        "    \"buy_total_amount_90\", \n",
        "    \"buy_total_quantity_90\", \n",
        "    \"buy_total_freq_90\", \n",
        "    \"buy_total_amount_120\", \n",
        "    \"buy_total_quantity_120\", \n",
        "    \"buy_total_freq_120\", \n",
        "    \"buy_total_amount_150\", \n",
        "    \"buy_total_quantity_150\", \n",
        "    \"buy_total_freq_150\", \n",
        "    \"buy_total_amount_180\", \n",
        "    \"buy_total_quantity_180\", \n",
        "    \"buy_total_freq_180\", \n",
        "    \"buy_company_freq\", \n",
        "    \"buy_company_amount\", \n",
        "    \"buy_company_avgamount\", \n",
        "    \"buy_company_quantity\", \n",
        "    \"buy_brand_freq\", \n",
        "    \"buy_brand_amount\", \n",
        "    \"buy_brand_avgamount\", \n",
        "    \"buy_brand_quantity\", \n",
        "    \"buy_category_freq\", \n",
        "    \"buy_category_amount\", \n",
        "    \"buy_category_avgamount\", \n",
        "    \"buy_category_quantity\", \n",
        "    \"buy_company_amount_30\", \n",
        "    \"buy_company_quantity_30\", \n",
        "    \"buy_company_freq_30\", \n",
        "    \"buy_brand_amount_30\", \n",
        "    \"buy_brand_quantity_30\", \n",
        "    \"buy_brand_freq_30\", \n",
        "    \"buy_category_amount_30\", \n",
        "    \"buy_category_quantity_30\", \n",
        "    \"buy_category_freq_30\", \n",
        "    \"buy_company_amount_60\", \n",
        "    \"buy_company_quantity_60\", \n",
        "    \"buy_company_freq_60\", \n",
        "    \"buy_brand_amount_60\", \n",
        "    \"buy_brand_quantity_60\", \n",
        "    \"buy_brand_freq_60\", \n",
        "    \"buy_category_amount_60\", \n",
        "    \"buy_category_quantity_60\", \n",
        "    \"buy_category_freq_60\", \n",
        "    \"buy_company_amount_90\", \n",
        "    \"buy_company_quantity_90\", \n",
        "    \"buy_company_freq_90\", \n",
        "    \"buy_brand_amount_90\", \n",
        "    \"buy_brand_quantity_90\", \n",
        "    \"buy_brand_freq_90\", \n",
        "    \"buy_category_amount_90\", \n",
        "    \"buy_category_quantity_90\", \n",
        "    \"buy_category_freq_90\", \n",
        "    \"buy_company_amount_120\", \n",
        "    \"buy_company_quantity_120\", \n",
        "    \"buy_company_freq_120\", \n",
        "    \"buy_brand_amount_120\", \n",
        "    \"buy_brand_quantity_120\", \n",
        "    \"buy_brand_freq_120\", \n",
        "    \"buy_category_amount_120\", \n",
        "    \"buy_category_quantity_120\", \n",
        "    \"buy_category_freq_120\", \n",
        "    \"buy_company_amount_150\", \n",
        "    \"buy_company_quantity_150\", \n",
        "    \"buy_company_freq_150\", \n",
        "    \"buy_brand_amount_150\", \n",
        "    \"buy_brand_quantity_150\", \n",
        "    \"buy_brand_freq_150\", \n",
        "    \"buy_category_amount_150\", \n",
        "    \"buy_category_quantity_150\", \n",
        "    \"buy_category_freq_150\", \n",
        "    \"buy_company_amount_180\", \n",
        "    \"buy_company_quantity_180\", \n",
        "    \"buy_company_freq_180\", \n",
        "    \"buy_brand_amount_180\", \n",
        "    \"buy_brand_quantity_180\", \n",
        "    \"buy_brand_freq_180\", \n",
        "    \"buy_category_amount_180\", \n",
        "    \"buy_category_quantity_180\", \n",
        "    \"buy_category_freq_180\", \n",
        "    \"buy_company_brand_category_freq\", \n",
        "    \"buy_company_brand_freq\", \n",
        "    \"buy_company_category_freq\", \n",
        "    \"buy_brand_category_freq\", \n",
        "    \"buy_company_daydiff\", \n",
        "    \"buy_brand_daydiff\", \n",
        "    \"buy_category_daydiff\", \n",
        "    \"buy_total_daydiff\"\n",
        "]"
      ],
      "metadata": {
        "id": "B2MsDYHuEYyc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from sklearn.preprocessing import StandardScaler\n",
        "trainStd = trainEncode.copy()\n",
        "X = trainEncode[select]\n",
        "scaler = StandardScaler()\n",
        "trainStd[select] = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "iF_hxsO1ClIU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "testStd = testEncode.copy()\n",
        "X = testEncode[select]\n",
        "scaler = StandardScaler()\n",
        "testStd[select] = scaler.fit_transform(X)"
      ],
      "metadata": {
        "id": "82zkXgtTFrbk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Generate Encoder"
      ],
      "metadata": {
        "id": "Xu-e73HwNzdB"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "target.to_csv('/content/drive/MyDrive/1經濟學/專題/featureNew/target.csv')\n",
        "targetT.to_csv('/content/drive/MyDrive/1經濟學/專題/featureNew/targetTrips.csv')\n",
        "trainData.to_csv('/content/drive/MyDrive/1經濟學/專題/featureNew/trainOriginal.csv')\n",
        "testData.to_csv('/content/drive/MyDrive/1經濟學/專題/featureNew/testOriginal.csv')\n",
        "trainEncode.to_csv('/content/drive/MyDrive/1經濟學/專題/featureNew/trainEncode.csv')\n",
        "testEncode.to_csv('/content/drive/MyDrive/1經濟學/專題/featureNew/testEncode.csv')\n",
        "targetEncode.to_csv('/content/drive/MyDrive/1經濟學/專題/featureNew/targetEncode.csv')\n",
        "trainStd.to_csv('/content/drive/MyDrive/1經濟學/專題/featureNew/trainStd.csv')\n",
        "testStd.to_csv('/content/drive/MyDrive/1經濟學/專題/featureNew/testStd.csv')"
      ],
      "metadata": {
        "id": "1mu1Ir1zDXik"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}